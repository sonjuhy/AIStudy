{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì´ 4ê°œì˜ zip íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 19831.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì´ 55ê°œì˜ zip íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:00<00:00, 54113.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  zip íŒŒì¼ì´ raw_zips í´ë”ë¡œ ëª¨ì˜€ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_zips(base_path, folder_name, target_folder_name=\"raw_zips\"):\n",
    "    target_dir = os.path.join(base_path, target_folder_name)\n",
    "    data_path = os.path.join(base_path,folder_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    zip_files = []\n",
    "    # 1. ëª¨ë“  í•˜ìœ„ í´ë”ë¥¼ ìˆœíšŒí•˜ë©° zip íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        # íƒ€ê²Ÿ í´ë” ìì²´ëŠ” ê²€ìƒ‰ì—ì„œ ì œì™¸\n",
    "        if target_folder_name in root:\n",
    "            continue\n",
    "            \n",
    "        for file in files:\n",
    "            if file.endswith(\".zip\"):\n",
    "                zip_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"ğŸ“¦ ì´ {len(zip_files)}ê°œì˜ zip íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 2. íŒŒì¼ ì´ë™\n",
    "    for file_path in tqdm(zip_files, desc=\"Moving files\"):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        dest_path = os.path.join(target_dir, file_name)\n",
    "        \n",
    "        # íŒŒì¼ ì´ë¦„ ì¶©ëŒ ë°©ì§€ (ì¤‘ë³µ ì´ë¦„ì´ ìˆì„ ê²½ìš°)\n",
    "        if os.path.exists(dest_path):\n",
    "            name, ext = os.path.splitext(file_name)\n",
    "            dest_path = os.path.join(target_dir, f\"{name}_dup{ext}\")\n",
    "            \n",
    "        shutil.move(file_path, dest_path)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "base_path = \"./DataSets/AIHub\"\n",
    "folder_list = [\"030.ì›¹ë°ì´í„°_ê¸°ë°˜_í•œêµ­ì–´_ë§ë­‰ì¹˜_ë°ì´í„°\", \"121.í•œêµ­ì–´_ì„±ëŠ¥ì´_ê°œì„ ëœ_ì´ˆê±°ëŒ€AI_ì–¸ì–´ëª¨ë¸_ê°œë°œ_ë°_ë°ì´í„°\"]\n",
    "for folder_name in folder_list:\n",
    "    collect_zips(base_path, folder_name)\n",
    "print(\"âœ… ëª¨ë“  zip íŒŒì¼ì´ raw_zips í´ë”ë¡œ ëª¨ì˜€ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì´ 59ê°œì˜ ì••ì¶• íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [05:13<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ì••ì¶• í•´ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "def smart_extract(zip_path, target_dir):\n",
    "    try:\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for file in zip_ref.namelist():\n",
    "                # [í•µì‹¬ ìˆ˜ì •] íŒŒì¼ëª… ì•ì˜ ìŠ¬ë˜ì‹œ(/)ë‚˜ ë°±ìŠ¬ë˜ì‹œ(\\)ë¥¼ ì œê±°í•˜ì—¬ ìƒëŒ€ ê²½ë¡œë¡œ ë§Œë“¦\n",
    "                clean_name = file.lstrip('/') \n",
    "                \n",
    "                # í•œê¸€ ê¹¨ì§ ë°©ì§€ ì²˜ë¦¬\n",
    "                try:\n",
    "                    decoded_name = clean_name.encode('cp437').decode('cp949')\n",
    "                except:\n",
    "                    decoded_name = clean_name\n",
    "                \n",
    "                target_path = os.path.join(target_dir, decoded_name)\n",
    "                \n",
    "                if file.endswith('/'):\n",
    "                    os.makedirs(target_path, exist_ok=True)\n",
    "                    continue\n",
    "                \n",
    "                # ì¶”ì¶œë  í´ë” ìƒì„± í™•ì¸\n",
    "                os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "                \n",
    "                # íŒŒì¼ ì“°ê¸°\n",
    "                with zip_ref.open(file) as source, open(target_path, \"wb\") as target:\n",
    "                    target.write(source.read())\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {os.path.basename(zip_path)} í•´ì œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "base_path = \"./DataSets/AIHub\"\n",
    "zip_folder = os.path.join(base_path, \"kor_xlarge_raw_zips\") # zip íŒŒì¼ë“¤ì´ ëª¨ì—¬ìˆëŠ” ê³³\n",
    "extract_base = os.path.join(base_path, \"kor_xlarge_extracted\")\n",
    "\n",
    "# 1. ëŒ€ìƒ í´ë” ìƒì„±\n",
    "os.makedirs(extract_base, exist_ok=True)\n",
    "\n",
    "# 2. ëª¨ë“  zip íŒŒì¼ ì°¾ì•„ í•´ì œ\n",
    "zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]\n",
    "print(f\"ğŸ“¦ ì´ {len(zip_files)}ê°œì˜ ì••ì¶• íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "corpus_624_name_list = [\"TL1.zip\", \"TS1.zip\", \"VL1.zip\", \"VS1.zip\"]\n",
    "for z_file in tqdm(zip_files, desc=\"Extracting\"):\n",
    "    # íŒŒì¼ëª…ì— ë”°ë¼ í•˜ìœ„ í´ë” ë¶„ë¥˜ (624 ë˜ëŠ” 71748 í‚¤ì›Œë“œë¡œ êµ¬ë¶„)\n",
    "    sub_folder = \"corpus_624\" if z_file in corpus_624_name_list else \"inst_71748\"\n",
    "    target_path = os.path.join(extract_base, sub_folder)\n",
    "    \n",
    "    smart_extract(os.path.join(zip_folder, z_file), target_path)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ì••ì¶• í•´ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f35de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON ì£¼ìš” í‚¤: dict_keys(['header', 'named_entity'])\n",
      "{\n",
      "  \"header\": {\n",
      "    \"identifier\": \"í…ìŠ¤íŠ¸_ì›¹ë°ì´í„° ë§ë­‰ì¹˜ êµ¬ì¶•__7000018002\",\n",
      "    \"name\": \"ì»¨í…Œì´ë„ˆ í…ìŠ¤íŠ¸ ì¸ì‹ì„ ìœ„í•œ í•™ìŠµìš© ë°ì´í„°ì…‹\",\n",
      "    \"category\": \"0\",\n",
      "    \"type\": \"0\",\n",
      "    \"source_file\": \"BWCU217000018002\",\n",
      "    \"source\": \"0\",\n",
      "    \"subject\": \"CU\"\n",
      "  },\n",
      "  \"named_entity\": [\n",
      "    {\n",
      "      \"title\": [\n",
      "        {\n",
      "          \"sentence\": \"ë¹…í†¤ (ì´ë¦„), ë¹„í˜„ì‹¤ì  í”¼ì§€ì»¬..í›ˆí›ˆí•¨ í•œë„ ì´ˆê³¼\",\n",
      "          \"labels\": []\n",
      "        }\n",
      "      ],\n",
      "      \"subtitle\": [],\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"sentence\": \"ê·¸ë£¹ ë¹…í†¤ ë©¤ë²„ (ì´ë¦„)ê°€ ë‚¨ë‹¤ë¥¸ í›ˆí›ˆí•¨ì„ ë½ëƒˆë‹¤..\",\n",
      "          \"labels\":\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "# ì¶”ì¶œëœ í´ë” ë‚´ì˜ JSON íŒŒì¼ í•˜ë‚˜ ì°¾ê¸°\n",
    "json_files = glob.glob(\"./DataSets/AIHub/kor_xlarge_extracted/**/*.json\", recursive=True)\n",
    "\n",
    "if json_files:\n",
    "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "        # í‚¤(Key) êµ¬ì¡° ì¶œë ¥\n",
    "        print(\"JSON ì£¼ìš” í‚¤:\", sample_data.keys())\n",
    "        # ë°ì´í„°ì˜ ì•„ì£¼ ì¼ë¶€ë§Œ ì¶œë ¥í•˜ì—¬ ë³¸ë¬¸ì´ ì–´ë”” ìˆëŠ”ì§€ í™•ì¸\n",
    "        print(json.dumps(sample_data, indent=2, ensure_ascii=False)[:500])\n",
    "else:\n",
    "    print(\"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed2ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì´ 63786ê°œ íŒŒì¼ì—ì„œ 'named_entity' êµ¬ì¡° ì¶”ì¶œ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63786/63786 [06:42<00:00, 158.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•© ì™„ë£Œ! ìƒì„±ëœ íŒŒì¼: /media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/AIHub/total_corpus.txt\n",
      "âœ… ìƒì„± ì™„ë£Œ! íŒŒì¼ í¬ê¸°: 11293.19 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_merge(base_dir, output_file):\n",
    "    json_files = glob.glob(os.path.join(base_dir, \"**/*.json\"), recursive=True)\n",
    "    print(f\"ğŸ“„ ì´ {len(json_files)}ê°œ íŒŒì¼ì—ì„œ 'named_entity' êµ¬ì¡° ì¶”ì¶œ ì‹œì‘...\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for file_path in tqdm(json_files):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                    data = json.load(f_in)\n",
    "                \n",
    "                texts = []\n",
    "\n",
    "                # í•µì‹¬: named_entity ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ\n",
    "                if 'named_entity' in data:\n",
    "                    for entity in data['named_entity']:\n",
    "                        # 1. title í•„ë“œì—ì„œ sentence ì¶”ì¶œ\n",
    "                        for section in ['title', 'content', 'subtitle']:\n",
    "                            if section in entity:\n",
    "                                for item in entity[section]:\n",
    "                                    if 'sentence' in item:\n",
    "                                        texts.append(item['sentence'])\n",
    "                \n",
    "                # ë§Œì•½ ìœ„ êµ¬ì¡°ê°€ ì•„ë‹ˆê³  ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì„ì—¬ìˆì„ ê²½ìš° ëŒ€ë¹„\n",
    "                elif 'document' in data:\n",
    "                    for doc in data['document']:\n",
    "                        for key in ['paragraph', 'utterance']:\n",
    "                            if key in doc:\n",
    "                                for p in doc[key]:\n",
    "                                    if 'form' in p: texts.append(p['form'])\n",
    "\n",
    "                # íŒŒì¼ì— ì“°ê¸°\n",
    "                clean_texts = [t.strip() for t in texts if t and len(t.strip()) > 0]\n",
    "                if clean_texts:\n",
    "                    f_out.write(\"\\n\".join(clean_texts) + \"\\n\")\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "extract_path = \"./DataSets/AIHub/kor_xlarge_extracted\"\n",
    "total_txt = \"./DataSets/AIHub/total_corpus.txt\"\n",
    "\n",
    "if os.path.exists(total_txt): os.remove(total_txt)\n",
    "\n",
    "extract_and_merge(extract_path, total_txt)\n",
    "print(f\"âœ… í†µí•© ì™„ë£Œ! ìƒì„±ëœ íŒŒì¼: {total_txt}\")\n",
    "\n",
    "if os.path.exists(total_txt):\n",
    "    size_mb = os.path.getsize(total_txt) / (1024**2)\n",
    "    print(f\"âœ… ìƒì„± ì™„ë£Œ! íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
